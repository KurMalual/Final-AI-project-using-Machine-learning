# -*- coding: utf-8 -*-
"""resume_screening.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11h1gO2KZvEgpb8UKErLh2OXrQQsoKpLB
"""

import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import pickle
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Load the resume dataset
raw_df = pd.read_csv('ResumeDataSet.csv')

# Create a copy of the dataset
df = raw_df.copy()

# Function to clean the text in the 'Resume' column
def clean_resume(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags
    text = re.sub(r'[^\w\s]', ' ', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove digits
    return text.strip()

# Apply normalization
df['cleaned_resume'] = df['Resume'].apply(clean_resume)
# Initialize a LabelEncoder to encode 'Category'
le = LabelEncoder()

"""# Data Exploration"""

# View first few rows
print(df.head())

# Dataset shape
print("Shape:", df.shape)

# Column types and non-null counts
print(df.info())

# Count missing values
print(df.isnull().sum())

# Unique job categories
print("Unique categories:", df['Category'].nunique())

# Count per category
print(df['Category'].value_counts())

plt.figure(figsize=(12,6))
sns.countplot(data=df, x='Category', order=df['Category'].value_counts().index)
plt.title("Resume Count per Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Add a column for resume word count
df['resume_length'] = df['Resume'].apply(lambda x: len(str(x).split()))

# Basic stats
print(df['resume_length'].describe())

# Plot distribution
sns.histplot(df['resume_length'], bins=20, kde=True)
plt.title("Distribution of Resume Lengths")
plt.xlabel("Number of Words")
plt.show()

# Example for one category
text = " ".join(df[df['Category'] == 'Data Science']['Resume'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("WordCloud for Data Science Resumes")
plt.show()

"""# Data Processing"""

# Remove exact duplicate rows
df = df.drop_duplicates()

# Optional: Remove duplicates in the 'resume' column only
df = df.drop_duplicates(subset='Resume')

# Normalization
def clean_resume(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags
    text = re.sub(r'[^\w\s]', ' ', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove digits
    return text.strip()

# Apply normalization
df['cleaned_resume'] = df['Resume'].apply(clean_resume)

# extra normalization
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])

df['cleaned_resume'] = df['cleaned_resume'].apply(remove_stopwords)

"""# Implement the Machine Learning Models"""

# 1. TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english', max_features=1000)
X = tfidf.fit_transform(df['cleaned_resume'])
y = df['Category']

# 2. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=100, stratify=y
)

# 3. Train Logistic Regression
log_model = LogisticRegression(max_iter=100)
log_model.fit(X_train, y_train)
log_preds = log_model.predict(X_test)

# 4. Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# 5. Evaluation Function
def evaluate_model(name, y_true, y_pred):
    print(f"\n=== {name} ===")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("\nClassification Report:\n", classification_report(y_true, y_pred, zero_division=1))
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# 6. Evaluate both models
evaluate_model("Logistic Regression", y_test, log_preds)
evaluate_model("Random Forest", y_test, rf_preds)